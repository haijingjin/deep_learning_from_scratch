[
  {
    "evaluation_cost": [
      0.6811018324003938,
      0.48252075177592635,
      0.39655978431673783,
      0.3417268906860376,
      0.3045662980179391,
      0.28415427515317254,
      0.25088286791243036,
      0.2266091297715379,
      0.21652751088803737,
      0.20289709663187477,
      0.19392016193898842,
      0.18431993527719578,
      0.18121133407995024,
      0.17043177986726524,
      0.16493949527473825,
      0.16470028894721284,
      0.16368714625326808,
      0.16826146638233794,
      0.1652747561830479,
      0.15964071036998723,
      0.15113670319170638,
      0.1581794587290447,
      0.14969589040954084,
      0.14700906358714746,
      0.14872684138995146,
      0.14636402228292414,
      0.15496871191596784,
      0.14447232186827902,
      0.14898168349886748,
      0.15114332793058002
    ],
    "evaluation_accuracy": [
      9031,
      9260,
      9394,
      9495,
      9557,
      9606,
      9626,
      9693,
      9692,
      9712,
      9718,
      9741,
      9755,
      9753,
      9758,
      9766,
      9772,
      9768,
      9761,
      9768,
      9773,
      9769,
      9784,
      9790,
      9795,
      9788,
      9780,
      9793,
      9782,
      9785
    ],
    "training_cost": [
      0.7243835206273835,
      0.5127148197744188,
      0.41652774554479083,
      0.34897323230492977,
      0.3024520052975948,
      0.27700256102959125,
      0.23716782376324386,
      0.20266194886876857,
      0.18401955385833896,
      0.16379927013854098,
      0.1535511140586876,
      0.13429932081592535,
      0.12746516909197736,
      0.1139032920129526,
      0.10543988981465675,
      0.09762347150690354,
      0.09058118230098587,
      0.09181102342601062,
      0.08155413753013213,
      0.07823440335432554,
      0.06895602106719548,
      0.06476994562417267,
      0.060336875077083775,
      0.055996174299063485,
      0.05081576937119449,
      0.04724791995475544,
      0.054037193883658054,
      0.04276565717517088,
      0.04266746597090162,
      0.03949346772146051
    ],
    "training_accuracy": [
      44672,
      46073,
      46815,
      47307,
      47743,
      47937,
      48239,
      48584,
      48732,
      48882,
      48992,
      49158,
      49208,
      49314,
      49396,
      49452,
      49493,
      49503,
      49561,
      49636,
      49668,
      49718,
      49738,
      49770,
      49799,
      49826,
      49797,
      49871,
      49857,
      49864
    ],
    "test_cost": 0.14592088313638518,
    "test_accuracy": 9794,
    "learning_rate_history": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "label": "no_lr_schedule",
    "architecture": [
      784,
      128,
      128,
      10
    ],
    "eta": 0.1,
    "batch_size": 20,
    "lmbda": 1.0,
    "regularization_type": "l2",
    "use_lr_schedule": false
  },
  {
    "evaluation_cost": [
      0.6816940151291966,
      0.4814605839962493,
      0.38582654322111726,
      0.33152996430303566,
      0.2948547460964218,
      0.27277391050730937,
      0.25364228274682027,
      0.2345126716538045,
      0.22548744054456502,
      0.1984827720922169,
      0.1923311787679774,
      0.18866111158210314,
      0.1872790082412473,
      0.17434563459785585,
      0.1722197990559662,
      0.16681311748098346,
      0.16120300946818478,
      0.15676343898388653,
      0.1553548827188938,
      0.1529872680241177,
      0.1515479950561472,
      0.15796365234165827,
      0.15092715290868647,
      0.16687589067874825,
      0.1533602669021634,
      0.14816460434708512,
      0.1430180679005049,
      0.14301658011829987,
      0.1458456488837479,
      0.14946635268924602
    ],
    "evaluation_accuracy": [
      9001,
      9269,
      9444,
      9511,
      9566,
      9605,
      9634,
      9662,
      9683,
      9724,
      9718,
      9740,
      9737,
      9763,
      9765,
      9767,
      9773,
      9781,
      9777,
      9780,
      9788,
      9779,
      9787,
      9761,
      9787,
      9797,
      9800,
      9805,
      9801,
      9808
    ],
    "training_cost": [
      0.7246499221011823,
      0.5106267984243679,
      0.4043859858926732,
      0.3411020058219814,
      0.29364150523612736,
      0.26426049110616745,
      0.23508003559363178,
      0.21241585009030375,
      0.19607964749487303,
      0.16332436133091122,
      0.15294894252134,
      0.14132070932159543,
      0.1374669447343501,
      0.12177595709423608,
      0.11006034497971622,
      0.10161569216188149,
      0.09296289472407952,
      0.08579071797934058,
      0.0792494800316689,
      0.07359689025085897,
      0.06976127293351667,
      0.07148223262949097,
      0.06151394027775361,
      0.06802834231297315,
      0.05821924045337846,
      0.050475517854966144,
      0.04803800029476222,
      0.04415654800292822,
      0.04121610260347362,
      0.043147550850529015
    ],
    "training_accuracy": [
      44694,
      46100,
      46976,
      47391,
      47782,
      48021,
      48296,
      48537,
      48643,
      48898,
      48993,
      49080,
      49139,
      49217,
      49345,
      49396,
      49458,
      49525,
      49588,
      49605,
      49654,
      49624,
      49706,
      49664,
      49755,
      49779,
      49800,
      49835,
      49857,
      49861
    ],
    "test_cost": 0.1465271814376292,
    "test_accuracy": 9798,
    "learning_rate_history": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "label": "with_lr_schedule",
    "architecture": [
      784,
      128,
      128,
      10
    ],
    "eta": 0.1,
    "batch_size": 20,
    "lmbda": 1.0,
    "regularization_type": "l2",
    "use_lr_schedule": true
  }
]